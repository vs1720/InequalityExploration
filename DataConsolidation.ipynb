{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alright, now we have to consolidate our data. \n",
    "\n",
    "### We have 3 major sources. The United Nations, the World Bank, and of course, our inequality data. We also have 2 sources which have the same format - we just treat this as its own source called Misc. data. Within each source the data and naming conventions are standardized, however, the conventions are different across sources. However we want to be able to join all of them together. Let's make an entity for each of the sources first, then we can worry about merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take the inequality index first and use that as our baseline. Because of naming inconsistencies, we are building a dictionary that gives us various country to code translations. Then, we introduce the 4 sources one at a time, sort out the countries which do not have a match, then go to the next one till as many countries as we can have corresponding codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.DataFrame\n",
    "path = \"DataSources\\\\swiiidGINI.csv\"\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.DataFrame\n",
    "path = \"DataSources\\\\codeTrans.csv\"\n",
    "codes = pd.read_csv(path)\n",
    "codes.columns\n",
    "codes[\"country\"] = codes[\"name\"]\n",
    "codes = codes[[\"country\", \"alpha-3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = codes.merge(data, on = \"country\", how = \"right\")\n",
    "miss = (set(temp[temp[\"alpha-3\"].isna()][\"country\"]))\n",
    "miss = ['Bolivia',\n",
    " 'Brunei',\n",
    " 'Cape Verde',\n",
    " 'Congo-Brazzaville',\n",
    " 'Congo-Kinshasa',\n",
    " 'Czech Republic',\n",
    " 'Czechoslovakia',\n",
    " 'Iran',\n",
    " 'Korea',\n",
    " 'Kosovo',\n",
    " 'Laos',\n",
    " 'Macedonia',\n",
    " 'Micronesia',\n",
    " 'Moldova',\n",
    " 'Palestinian Territories',\n",
    " 'Russia',\n",
    " 'Soviet Union',\n",
    " 'St. Kitts and Nevis',\n",
    " 'St. Lucia',\n",
    " 'St. Vincent and Grenadines',\n",
    " 'Swaziland',\n",
    " 'Syria',\n",
    " 'São Tomé and Príncipe',\n",
    " 'Taiwan',\n",
    " 'Tanzania',\n",
    " 'United Kingdom',\n",
    " 'United States',\n",
    " 'Venezuela',\n",
    " 'Vietnam',\n",
    " 'Yugoslavia']\n",
    "#Source https://laendercode.net/en/3-letter-list.html\n",
    "# note that korea is SOuth Korea\n",
    "codes = [\"BOL\", \"BRN\", \"CPV\", \"COG\", \"COD\", \"CZE\", \"NO1\", \"IRN\", \"KOR\", \"XKX\", \"LAO\", \"MKD\", \"FSM\", \"MDA\", \"PSE\", \"RUS\", \n",
    "        \"NO2\", \"KNA\", \"LCA\", \"VCT\", \"SWZ\", \"SYR\", \"STP\", \"TWN\", \"TZA\", \"GBR\", \"USA\", \"VEN\", \"VNM\", \"YUG\"]\n",
    "tc = {}\n",
    "for cnt in range(len(codes)):\n",
    "    #print(miss[cnt])\n",
    "    tc[miss[cnt]] = codes[cnt]\n",
    "for key, val in tc.items():\n",
    "    temp.loc[temp[\"country\"] == key, \"alpha-3\"] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {y:x for x,y in zip(temp[\"country\"],temp[\"alpha-3\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_csv(path)\n",
    "codes.columns\n",
    "codes[\"country\"] = codes[\"name\"]\n",
    "codes = codes[[\"country\", \"alpha-3\"]]\n",
    "d2 = {y:x for x,y in zip(codes[\"country\"],codes[\"alpha-3\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dd = defaultdict(list)\n",
    "\n",
    "for d in (d1, d2): # you can list as many input dicts as you want here\n",
    "    for key, value in d.items():\n",
    "        if value not in dd[key]:\n",
    "            dd[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code',\n",
       "       '1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968',\n",
       "       '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977',\n",
       "       '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986',\n",
       "       '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',\n",
       "       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004',\n",
       "       '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013',\n",
       "       '2014', '2015', '2016', '2017', '2018', '2019'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"DataSources\\\\trade.csv\"\n",
    "data = pd.read_csv(path, header = 4)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcnts = set(data[\"Country Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually added to our dictionary as data was added. Unadded countries were checked with something like:\n",
    "\n",
    "#### set(data3[data3[\"code\"].isna()][\"country\"])\n",
    "\n",
    "## After applying the mapping in our dictionary. Then we decided whether to keep it or not (for instance, it is pointless to keep west gemany since out time period focus is after it exen existed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = {}\n",
    "for key,val in dd.items():\n",
    "    for x in val:\n",
    "        rdd[x] = key\n",
    "\n",
    "rdd['Yemen North'] = \"YEM\"\n",
    "rdd['Myanmar (Burma)'] = \"MMR\"\n",
    "rdd[\"Moldova (Republic of)\"] = \"MDA\"\n",
    "rdd[\"Korea (Democratic People's Rep. of)\"] = \"PRK\"\n",
    "rdd[\"Korea (Republic of)\"] = \"KOR\"\n",
    "rdd[\"The former Yugoslav Republic of Macedonia\"] = \"MKD\"\n",
    "rdd[\"Hong Kong, China (SAR)\"] = \"HKG\"\n",
    "rdd[\"Tanzania (United Republic of)\"] = \"TZA\"\n",
    "rdd[\"Eswatini (Kingdom of)\"] = \"SWZ\"\n",
    "rdd[\"Congo (Democratic Republic of the)\"] = \"COD\"\n",
    "rdd[\"Korea North\"] = \"PRK\"\n",
    "rdd[\"Slovak Republic\"] = \"SVK\"\n",
    "rdd[\"Dominican Rep\"] = \"DOM\"\n",
    "rdd[\"Bosnia\"] = \"BOS\"\n",
    "rdd[\"Serbia and Montenegro\"] = \"YUG\"\n",
    "rdd[\"Yugoslavia\"] = \"YUG\"\n",
    "rdd[\"Congo Brazzaville\"] = \"COG\"\n",
    "rdd[\"UAE\"] = \"UAE\"\n",
    "rdd[\"Ivory Coast\"] = \"CIV\"\n",
    "rdd[\"Korea South\"] = \"KOR\"\n",
    "rdd[\"Egypt, Arab Rep.\"] = \"EGY\"\n",
    "rdd[\"Congo Kinshasa\"] = \"COD\"\n",
    "rdd['East Timor'] = \"TLS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here 2 dataframes will act as bases for our tables in sql. country_code and country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = pd.DataFrame(list(rdd.items()), columns = [\"cntr_name\", \"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_table = pd.DataFrame(list(set(rdd.values())), columns = [\"code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make the CSVs and put it in SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by making baseline inequality\n",
    "\n",
    "### That is, we need to fill in missing indices. That is for instance, if we do not have data for Bolivia in 2008, we still need an empty row representing Bolivia 2008. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.DataFrame\n",
    "path = \"DataSources\\\\swiiidGINI.csv\"\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "codes = pd.DataFrame\n",
    "path = \"DataSources\\\\codeTrans.csv\"\n",
    "codes = pd.read_csv(path)\n",
    "\n",
    "codes.columns\n",
    "codes[\"country\"] = codes[\"name\"]\n",
    "codes = codes[[\"country\", \"alpha-3\"]]\n",
    "\n",
    "data[\"code\"] = data[\"country\"].map(rdd)\n",
    "temp = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Viral Shanker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Viral Shanker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Viral Shanker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Viral Shanker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "cntrConv = list(set(list(zip(temp[\"country\"], temp[\"code\"]))))\n",
    "\n",
    "masterDict = {}\n",
    "masterDict[\"country\"] = []\n",
    "masterDict[\"code\"] = []\n",
    "masterDict[\"year\"] = []\n",
    "masterDict[\"gini_disp\"] = []\n",
    "masterDict[\"gini_disp_se\"] = []\n",
    "masterDict[\"gini_mkt\"] = []\n",
    "masterDict[\"gini_mkt_se\"] = []\n",
    "for x in cntrConv:\n",
    "    for year in range(1996, 2017):\n",
    "        masterDict[\"country\"].append(x[0])\n",
    "        masterDict[\"code\"].append(x[1])\n",
    "        masterDict[\"year\"].append(year)\n",
    "        try:\n",
    "            masterDict[\"gini_disp\"].append(temp[temp[\"code\"] == x[1]][temp[\"year\"] == year][\"gini_disp\"].values[0])\n",
    "        except IndexError:\n",
    "            masterDict[\"gini_disp\"].append(np.nan)\n",
    "        try:\n",
    "            masterDict[\"gini_disp_se\"].append(temp[temp[\"code\"] == x[1]][temp[\"year\"] == year][\"gini_disp_se\"].values[0])\n",
    "        except IndexError:\n",
    "            masterDict[\"gini_disp_se\"].append(np.nan)\n",
    "        try:\n",
    "            masterDict[\"gini_mkt\"].append(temp[temp[\"code\"] == x[1]][temp[\"year\"] == year][\"gini_mkt\"].values[0])\n",
    "        except IndexError:\n",
    "            masterDict[\"gini_mkt\"].append(np.nan)\n",
    "        try:\n",
    "            masterDict[\"gini_mkt_se\"].append(temp[temp[\"code\"] == x[1]][temp[\"year\"] == year][\"gini_mkt_se\"].values[0])\n",
    "        except IndexError:\n",
    "            masterDict[\"gini_mkt_se\"].append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "master =  pd.DataFrame.from_dict(masterDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's get the other data from the world bank and the UN. The trick is, the data is given differently. There is Country - indicator, and then the columns are years. We must transpose and seperate! \n",
    "\n",
    "### First let's make some utility functions to make adding painless. We will use our dictionary rdd do convert country names into codes for the UN. For the World bank, we're lucky and the code is included in the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## The ren flag is activated for specific World Bank data which names years slightly differently\n",
    "def addWorldBankVar(data, varName, ren = False):\n",
    "    data[\"code\"] = data[\"Country Code\"]\n",
    "    if ren:\n",
    "        data.columns = ['Country Name', 'Country Code', 'Series Name', 'Series Code',\n",
    "       '1996', '1998', '2000', '2002',\n",
    "       '2003', '2004', '2005', '2006',\n",
    "       '2007', '2008', '2009', '2010',\n",
    "       '2011', '2012', '2013', '2014',\n",
    "       '2015', '2016', '2017', '2018', \"code\"]\n",
    "        yrs = ['1996', '1998', '2000', '2002',\n",
    "       '2003', '2004', '2005', '2006',\n",
    "       '2007', '2008', '2009', '2010',\n",
    "       '2011', '2012', '2013', '2014',\n",
    "       '2015', '2016', '2017', '2018']\n",
    "        data = data[[\"code\"] + yrs]\n",
    "        data = data.replace(\"..\", np.nan)\n",
    "    else:\n",
    "        data = data[[\"code\"] + [str(x) for x in list(range(1996,2017))]]\n",
    "    data = pd.melt(data, id_vars = [\"code\"], var_name = \"year\", value_name = varName)\n",
    "    data = data.sort_values([\"code\", \"year\"])\n",
    "    data[\"year\"] = np.int_(data[\"year\"])\n",
    "    data = data[data[\"code\"].isin(master[\"code\"])]\n",
    "    data = data.set_index(['code','year'], drop = True)\n",
    "    return data\n",
    "\n",
    "def addUNData(data, varName):\n",
    "    data = data.replace(\"..\", np.nan)\n",
    "    data[\"code\"] = data[\"Country\"].map(rdd)\n",
    "    data = data[[\"code\"] + [str(x) for x in list(range(1996,2017))]]\n",
    "    data = pd.melt(data, id_vars = [\"code\"], var_name = \"year\", value_name = varName)\n",
    "    data = data.sort_values([\"code\", \"year\"])\n",
    "    data[\"year\"] = np.int_(data[\"year\"])\n",
    "    data = data[data[\"code\"].isin(master[\"code\"])]\n",
    "    data = data.set_index(['code','year'], drop = True)\n",
    "    return data\n",
    "    \n",
    "def addNormalData(data, varName):\n",
    "    data = data.sort_values([\"code\", \"year\"])\n",
    "    data[\"year\"] = np.int_(data[\"year\"])\n",
    "    data = data[data[\"year\"].isin(list(range(1996,2017)))]\n",
    "    data = data[data[\"code\"].isin(master[\"code\"])]\n",
    "    data = data.set_index(['code','year'], drop = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 4 UN variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annoyingly, these are in LATIN-1 instead of UTF-8, hence we need the argument\n",
    "\n",
    "path = \"DataSources\\\\hdi.csv\"\n",
    "data1 = pd.read_csv(path, encoding = \"LATIN-1\", header = 1)\n",
    "\n",
    "path = \"DataSources\\\\EDU_IDX.csv\"\n",
    "data2 = pd.read_csv(path, encoding = \"LATIN-1\", header = 1)\n",
    "\n",
    "path = \"DataSources\\\\Income_Index.csv\"\n",
    "data3 = pd.read_csv(path, encoding = \"LATIN-1\", header = 1)\n",
    "\n",
    "path = \"DataSources\\\\life_expect.csv\"\n",
    "data4 = pd.read_csv(path, encoding = \"LATIN-1\", header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNmaster = addUNData(data1, \"HDI\").join(addUNData(data2, \"EDU_IDX\")).join(addUNData(data3, \"Income_Index\")).join(addUNData(data4, \"life\"))\n",
    "UNmaster = UNmaster.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the World Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First there is the complicated World Bank one. This one hhas multiple indicators\n",
    "path = \"DataSources\\\\WorldBankInds.csv\"\n",
    "data = pd.read_csv(path)\n",
    "snames = list(set(data[\"Series Name\"].dropna()))\n",
    "snames = [x for x in snames if (\"Estimate\" in x) or (\"Standard\" in x)] #We're just concerend with the errors and the estimates\n",
    "snamesActual = [\"corruptionSE\", \"ruleOfLawSE\", \"voice\", \"regQual\", \"govEffect\", \"govEffectSE\", \"stabilitySE\", \"voiceSE\", \n",
    "               \"corruption\", \"stability\", \"regQualSE\", \"ruleOfLaw\"]\n",
    "rename = zip(snames, snamesActual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Viral Shanker\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "masterWB = addWorldBankVar(data[data[\"Series Name\"] == snames[0]], snamesActual[0], True)\n",
    "\n",
    "for cnt,name in enumerate(snames[1:]):\n",
    "    datat = data[data[\"Series Name\"] == name]\n",
    "    datat = addWorldBankVar(datat, snamesActual[cnt+1], True)\n",
    "    masterWB = masterWB.join(datat)\n",
    "    \n",
    "masterWB = masterWB.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are easy single indicator adds \n",
    "data = pd.DataFrame\n",
    "path = \"DataSources\\\\GDPcapitaGrowth.csv\"\n",
    "data = pd.read_csv(path, header = 4)\n",
    "\n",
    "data2 = pd.DataFrame\n",
    "path = \"DataSources\\\\GDPcap.csv\"\n",
    "data2 = pd.read_csv(path, header = 4)\n",
    "data = addWorldBankVar(data, \"INCOMEPC_growth\")\n",
    "data2 = addWorldBankVar(data2, \"INCOMEPC\")\n",
    "masterWB = masterWB.join(data)\n",
    "masterWB = masterWB.join(data2)\n",
    "\n",
    "data = pd.DataFrame\n",
    "path = \"DataSources\\\\trade.csv\"\n",
    "data = pd.read_csv(path, header = 4)\n",
    "data = addWorldBankVar(data, \"trade\")\n",
    "masterWB = masterWB.join(data)\n",
    "\n",
    "data = pd.DataFrame\n",
    "path = \"DataSources\\\\industrial.csv\"\n",
    "data = pd.read_csv(path, header = 4)\n",
    "data = addWorldBankVar(data, \"industrial\")\n",
    "masterWB = masterWB.join(data)\n",
    "\n",
    "data = pd.DataFrame\n",
    "path = \"DataSources\\\\manu.csv\"\n",
    "data = pd.read_csv(path, header = 4)\n",
    "data = addWorldBankVar(data, \"manu\")\n",
    "masterWB = masterWB.join(data)\n",
    "\n",
    "data = pd.DataFrame\n",
    "path = \"DataSources\\\\agri.csv\"\n",
    "data = pd.read_csv(path, header = 4)\n",
    "data = addWorldBankVar(data, \"agri\")\n",
    "masterWB = masterWB.join(data)\n",
    "\n",
    "data = pd.DataFrame\n",
    "path = \"DataSources\\\\URBAN.csv\"\n",
    "data = pd.read_csv(path, header = 4)\n",
    "data = addWorldBankVar(data, \"urban\")\n",
    "masterWB = masterWB.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, a table with some misc. variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DataSources\\\\financeOpen.csv\"\n",
    "data3 = pd.read_csv(path)\n",
    "data3 = addNormalData(data3, \"financeOpen\")[[\"kaopen\"]]\n",
    "masterMisc = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DataSources\\\\Polity.csv\"\n",
    "data3 = pd.read_csv(path)\n",
    "data3[\"code\"] = data3[\"country\"].map(rdd)\n",
    "data3 = data3[data3[\"code\"].isin(rdd.values())]\n",
    "data3 = addNormalData(data3, \"polity\")[[\"polity2\"]]\n",
    "masterMisc = masterMisc.join(data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, Finally, the last table we need to ensure normalization: code_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr_year = []\n",
    "ac = list(set(rdd.values()))\n",
    "y = list(range(1996,2019))\n",
    "for x in ac:\n",
    "    for b in y:\n",
    "        cntr_year.append([x,b])\n",
    "code_year = pd.DataFrame(cntr_year, columns = [\"code\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add our id to every table being in format code_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_year[\"id\"] = code_year[\"code\"].astype(str) + \"_\" + code_year[\"year\"].astype(str)\n",
    "master[\"g_id\"] = master[\"code\"].astype(str) + \"_\" + master[\"year\"].astype(str)\n",
    "UNmaster = UNmaster.assign(**UNmaster.index.to_frame())\n",
    "UNmaster[\"un_id\"] = UNmaster[\"code\"].astype(str) + \"_\" + UNmaster[\"year\"].astype(str)\n",
    "masterWB = masterWB.assign(**masterWB.index.to_frame())\n",
    "masterWB[\"wb_id\"] = masterWB[\"code\"].astype(str) + \"_\" + masterWB[\"year\"].astype(str)\n",
    "masterMisc = masterMisc.assign(**masterMisc.index.to_frame())\n",
    "masterMisc[\"misc_id\"] = masterMisc[\"code\"].astype(str) + \"_\" + masterMisc[\"year\"].astype(str)\n",
    "master.drop(columns = [\"code\", \"year\"], inplace = True)\n",
    "UNmaster.drop(columns = [\"code\", \"year\"], inplace = True)\n",
    "masterWB.drop(columns = [\"code\", \"year\"], inplace = True)\n",
    "masterMisc.drop(columns = [\"code\", \"year\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now to get these into SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our engine SQLalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"init.txt\") as jf:\n",
    "    inp = json.load(jf)\n",
    "\n",
    "accessString = \"postgresql://\" + inp[\"username\"] + \":\" + inp[\"password\"] + \"@\" + inp[\"hostname\"] + \"/\" + inp[\"database\"]\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(accessString, echo = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To completely reset our database uncomment the below line \n",
    "#engine.execute(\"DROP SCHEMA public CASCADE; CREATE SCHEMA public; GRANT ALL ON SCHEMA public TO postgres; GRANT ALL ON SCHEMA public TO public; COMMENT ON SCHEMA public IS 'standard public schema';\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simply go from pandas dataframe to table and add constraints as given in the ER diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99e945438>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"DROP TABLE IF EXISTS codes_table\")\n",
    "codes_table.to_sql('codes_table', engine, index = False)\n",
    "engine.execute(\"ALTER TABLE codes_table ADD PRIMARY KEY (code)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f0acb38>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"DROP TABLE IF EXISTS country_code\")\n",
    "country_code.to_sql('country_code', engine, index = False)\n",
    "engine.execute(\"ALTER TABLE country_code ADD PRIMARY KEY (cntr_name)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99d4c3c88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"ALTER TABLE country_code ADD FOREIGN KEY (code) REFERENCES codes_table (code);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f0accf8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"DROP TABLE IF EXISTS code_year\")\n",
    "code_year.to_sql('code_year', engine, index = False)\n",
    "engine.execute(\"ALTER TABLE code_year ADD PRIMARY KEY (id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f0ac828>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"ALTER TABLE code_year ADD FOREIGN KEY (code) REFERENCES codes_table (code);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f0c99e8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"DROP TABLE IF EXISTS master_gini\")\n",
    "master.to_sql('master_gini', engine, index = False)\n",
    "engine.execute(\"ALTER TABLE master_gini ADD PRIMARY KEY (g_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f10eda0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"DROP TABLE IF EXISTS master_un\")\n",
    "UNmaster.to_sql('master_un', engine, index = False)\n",
    "engine.execute(\"ALTER TABLE master_un ADD PRIMARY KEY (un_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f103550>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"DROP TABLE IF EXISTS master_wb\")\n",
    "masterWB.to_sql('master_wb', engine, index = False)\n",
    "engine.execute(\"ALTER TABLE master_wb ADD PRIMARY KEY (wb_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f10eac8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"DROP TABLE IF EXISTS master_misc\")\n",
    "masterMisc.to_sql('master_misc', engine, index = False)\n",
    "engine.execute(\"ALTER TABLE master_misc ADD PRIMARY KEY (misc_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f0c95c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"ALTER TABLE master_gini ADD FOREIGN KEY (g_id) REFERENCES code_year (id);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f0ff438>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"ALTER TABLE master_un ADD FOREIGN KEY (un_id) REFERENCES code_year (id);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f0ffba8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"ALTER TABLE master_wb ADD FOREIGN KEY (wb_id) REFERENCES code_year (id);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2a99f0ffef0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"ALTER TABLE master_misc ADD FOREIGN KEY (misc_id) REFERENCES code_year (id);\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL ENTITIES MADE!\n",
    "\n",
    "### We have our relational databases set up! Next python notebook we will get down to the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (intro-to-ml)",
   "language": "python",
   "name": "intro-to-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
